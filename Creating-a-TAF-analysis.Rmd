---
output: github_document
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
source("R/utilities.R")
```

See also:
[Bib entries](Bib-entries),
[Example data records](Example-data-records).

This page was based on using the `icesTAF` package version ```r packageVersion("icesTAF")```
dated ```r packageDate("icesTAF")```.

## In this guide

* [Clone](#clone)
* [Make Skeleton](#make-skeleton)
* [Upload initial data](#upload-initial-data)
  - [Upload files](#upload-files)
  - [Make data available](#make-data-available-to-assessment)

## Clone

The first step is asking the ICES secratariat to create a repository for your stock.  In this example we will
work with North Sea cod: `r repoName` in 2019 - the repository name for the assessment will be
`r repoName`.  When a new repository is created on GitHub you should clone the repository to your
own computer.  From here we will assume you have cloned the repository.

## Make skeleton

The first step in creating a TAF analysis is to set out the basic folder and file structure of the project.
This is done using the function `r taflink("taf.skeleton")`,
which creates the following structure in your working directory

```{r taf-skeleton-output, results='asis', echo=FALSE}
# make skeleton
taf.skeleton(repoName, force = TRUE)

# get skeleton path structure
paths <- dir(repoName, full.names = TRUE, recursive = TRUE)
paths <- c(paste0(repoName, "/bootstrap/initial/data/"), paths)
printDir(paths)
```

## Upload initial data

The next step is to set up the data requirements for your assessment.  There are
three ways to get data into an assessment: (1) upload files directly, (2) get a
file from the web, and (3) use R code to access a web service.  We will cover
1 and 2 here.  See [Example data records](Example-data-records)
for more information.

### Upload files

To upload a file called `catch.csv` - the contents might be something like:

```
year,catches
2015,2109
2016,3455
2017,3466
2018,2050
```

Simply copy the file in to the `bootstrap/initial/data` folder.  The directory structure will now look like this:

```{r data-file, results='asis', echo=FALSE}
cat(
  "year,catches",
  "2015,2109",
  "2016,3455",
  "2017,3466",
  "2018,2050",
  sep = "\n",
  file = paste0(repoName, "/bootstrap/initial/data/catch.csv")
)
printDir()
```

### Make data available to assessment

So far we have uploaded data into the intial data folder, but to make it available to the assessment
it needs to be in the `bootstrap/data` folder, and the only way to do this is
to create a file reference and then run the function
`r taflink("taf.bootstrap")`. To reference the data we create a file called
`DATA.bib` using the helper function `r taflink("draft.data")`, and add a few
fields:

```{r draft-data, echo=2}
setwd(repoName)
draft.data(
  originator = "WGNSSK",
  year = 2019,
  title = "Catch data for cod.27.347d",
  period = "2015-2018"
)
setwd("..")
```

Then we write this to the `DATA.bib` file by specifying `file = TRUE`:

```{r draft-data2, echo=2}
setwd(repoName)
draft.data(
  originator = "WGNSSK",
  year = 2019,
  title = "Catch data for cod.27.347d",
  period = "2015-2018",
  file = TRUE
)
setwd("..")
```

Finally we run `taf.bootstrap()` to get the data into the `bootstrap/data` folder, leaving the directory tree
looking like this:

```{r taf-bootstrap, echo=FALSE, results='asis', message=FALSE}
setwd(repoName)
taf.bootstrap()
setwd("..")
printDir()
```

## Preprocess data, write TAF data tables

Now that you have created the file structure and uploaded your data, it's time to turn your attention to the `data.R` script. The  purpose of this script is to do some data
processing. This could be (among other things):

* calculating a plus group,
* taking averages of survey indices,
* calculating fill-in values and removing outliers,
* calculating smoothed time series

The `data.R` script should also write out input data into flat `.csv` files, so they are readable and reviewable by others.

### Preprocessing

### Writing TAF tables
